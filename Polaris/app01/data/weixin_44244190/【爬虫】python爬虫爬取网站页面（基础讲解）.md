
--- 
title:  ã€çˆ¬è™«ã€‘pythonçˆ¬è™«çˆ¬å–ç½‘ç«™é¡µé¢ï¼ˆåŸºç¡€è®²è§£ï¼‰ 
tags: []
categories: [] 

---
>  
 <img src="https://img-blog.csdnimg.cn/6e2c8c7bccdc41cd911dc26a692693a2.jpeg" alt="è¯·æ·»åŠ å›¾ç‰‡æè¿°"> 
 <hr> 
 ğŸ‘‰åš__ä¸»ğŸ‘ˆï¼šç±³ç æ”¶å‰²æœº ğŸ‘‰æŠ€__èƒ½ğŸ‘ˆï¼šC++/Pythonè¯­è¨€ ğŸ‘‰å…¬ä¼—å·ğŸ‘ˆï¼šæµ‹è¯•å¼€å‘è‡ªåŠ¨åŒ–ã€è·å–æºç +å•†ä¸šåˆä½œã€‘ ğŸ‘‰è£__èª‰ğŸ‘ˆï¼šé˜¿é‡Œäº‘åšå®¢ä¸“å®¶åšä¸»ã€51CTOæŠ€æœ¯åšä¸» ğŸ‘‰ä¸“__æ³¨ğŸ‘ˆï¼šä¸“æ³¨ä¸»æµæœºå™¨äººã€äººå·¥æ™ºèƒ½ç­‰ç›¸å…³é¢†åŸŸçš„å¼€å‘ã€æµ‹è¯•æŠ€æœ¯ã€‚ 


>  
 <h2>çˆ¬è™«ï¼ˆæ¡†æ¶ï¼‰çˆ¬å–ç½‘ç«™é¡µé¢</h2> 
 <hr> 
  
  
  <h4>ç›®å½•</h4> 
  - - - <ul><li><ul><li>- - - - - -  
   </li></ul> 
   </li>- </ul> 
  
  


## çˆ¬è™«ï¼ˆæ¡†æ¶ï¼‰çˆ¬å–ç½‘ç«™é¡µé¢

#### 1. å¯¼å…¥å¿…è¦çš„åº“

```
import requests
from bs4 import BeautifulSoup

```
- `requests`åº“ç”¨äºå‘é€HTTPè¯·æ±‚ä»¥è·å–ç½‘é¡µå†…å®¹ã€‚- `BeautifulSoup`åº“ç”¨äºè§£æHTMLå†…å®¹å¹¶æå–æˆ‘ä»¬éœ€è¦çš„ä¿¡æ¯ã€‚
#### 2. è·å–ç½‘é¡µå†…å®¹

æˆ‘ä»¬é¦–å…ˆè¦ä½¿ç”¨`requests`åº“è·å–é¡µé¢çš„HTMLå†…å®¹ã€‚

```
url = 'https://example.com/articles'
response = requests.get(url)
html_content = response.content

```

#### 3. ä½¿ç”¨BeautifulSoupè§£æHTML

å°†è·å–åˆ°çš„HTMLå†…å®¹ä¼ é€’ç»™`BeautifulSoup`ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç”¨å®ƒæ¥è§£æé¡µé¢äº†ã€‚

```
soup = BeautifulSoup(html_content, 'html.parser')

```

#### 4. æ•°æ®æå–

è¿™å®Œå…¨å–å†³äºä½ æƒ³ä»é¡µé¢ä¸­æå–å“ªäº›ä¿¡æ¯ã€‚å‡è®¾æˆ‘ä»¬è¦æå–æ‰€æœ‰æ–‡ç« æ ‡é¢˜å’Œé“¾æ¥ï¼š

```
articles = soup.find_all('div', class_='article')  # å‡è®¾æ¯ç¯‡æ–‡ç« éƒ½åŒ…å«åœ¨ä¸€ä¸ªclassä¸º'article'çš„divæ ‡ç­¾å†…

for article in articles:
    title = article.find('h2').text
    link = article.find('a')['href']
    print(title, link)

```

#### 5. å¼‚å¸¸å¤„ç†

åœ¨çˆ¬å–ç½‘ç«™æ—¶å¯èƒ½ä¼šé‡åˆ°å„ç§é—®é¢˜ï¼Œå¦‚ç½‘ç»œé—®é¢˜ã€é¡µé¢ä¸å­˜åœ¨ç­‰ã€‚æˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€äº›å¼‚å¸¸å¤„ç†æ¥ç¡®ä¿çˆ¬è™«çš„ç¨³å®šæ€§ã€‚

```
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()  # å¦‚æœå“åº”çŠ¶æ€ä¸æ˜¯200ï¼Œåˆ™å¼•å‘å¼‚å¸¸
except requests.RequestException as e:
    print(f"Error fetching the url: {<!-- -->url}. Reason: {<!-- -->e}")

```

#### 6. é¿å…è¢«å°ç¦

å½“è¿ç»­å¹¶é¢‘ç¹è¯·æ±‚æŸä¸ªç½‘ç«™æ—¶ï¼Œå¯èƒ½ä¼šè¢«å°ç¦ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥é¿å…è¿™ç§æƒ…å†µï¼š
1. **è®¾ç½®User-Agent**ï¼šä¼ªè£…æˆçœŸæ­£çš„æµè§ˆå™¨ã€‚1. **è®¾ç½®å»¶è¿Ÿ**ï¼šåœ¨è¿ç»­çš„è¯·æ±‚ä¹‹é—´è®¾ç½®å»¶è¿Ÿï¼Œä¾‹å¦‚ä½¿ç”¨`time.sleep(5)`å»¶è¿Ÿ5ç§’ã€‚1. **ä½¿ç”¨ä»£ç†**ï¼šä½¿ç”¨ä¸åŒçš„IPåœ°å€æ¥å‘é€è¯·æ±‚ã€‚
#### å®Œæ•´ä»£ç ç¤ºä¾‹ï¼š

```
import requests
from bs4 import BeautifulSoup
import time

url = 'https://example.com/articles'  # æ¢æˆä½ çš„ç½‘ç«™

headers = {<!-- -->
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

try:
    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'html.parser')

    articles = soup.find_all('div', class_='article')

    for article in articles:
        title = article.find('h2').text
        link = article.find('a')['href']
        print(title, link)
        time.sleep(5)  # æ¯æŠ“å–ä¸€ä¸ªé¡µé¢åï¼Œæš‚åœ5ç§’

except requests.RequestException as e:
    print(f"Error fetching the url: {<!-- -->url}. Reason: {<!-- -->e}")

```

## æ³¨æ„äº‹é¡¹

**æ³¨æ„**ï¼šåœ¨è¿è¡Œçˆ¬è™«ä¹‹å‰ï¼Œä½ åº”è¯¥ï¼š
1. æ£€æŸ¥ç›®æ ‡ç½‘ç«™çš„`robots.txt`æ–‡ä»¶ï¼Œäº†è§£å“ªäº›é¡µé¢å…è®¸çˆ¬å–ã€‚1. ä¸è¦é¢‘ç¹åœ°è¯·æ±‚ç½‘ç«™ï¼Œè¿™å¯èƒ½ä¼šè¢«è§†ä¸ºDDoSæ”»å‡»ã€‚1. ç¡®ä¿ä½ æœ‰æƒçˆ¬å–å’Œä½¿ç”¨ç›®æ ‡æ•°æ®ã€‚1. è€ƒè™‘ç½‘ç«™çš„è´Ÿè½½ï¼Œä¸è¦å¯¹ç½‘ç«™é€ æˆä¸å¿…è¦çš„å‹åŠ›ã€‚1. æœ‰æ—¶å€™ï¼Œä½¿ç”¨APIæ˜¯è·å–æ•°æ®çš„æ›´å¥½æ–¹æ³•ï¼Œè®¸å¤šç½‘ç«™æä¾›APIæ¥è·å–æ•°æ®ï¼Œè€Œä¸æ˜¯ç›´æ¥çˆ¬å–ã€‚