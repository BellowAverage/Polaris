
--- 
title:  é©¾é©­AIç»˜ç”»ï¼šã€ŠAIé­”æ³•ç»˜ç”»ã€‹å¸¦ä½ ç§’å˜é¡¶çº§ç”»æ‰‹ï¼ 
tags: []
categories: [] 

---
â€ƒâ€ƒå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯herosunlyã€‚985é™¢æ ¡ç¡•å£«æ¯•ä¸šï¼Œç°æ‹…ä»»ç®—æ³•ç ”ç©¶å‘˜ä¸€èŒï¼Œçƒ­è¡·äºæœºå™¨å­¦ä¹ ç®—æ³•ç ”ç©¶ä¸åº”ç”¨ã€‚æ›¾è·å¾—é˜¿é‡Œäº‘å¤©æ± æ¯”èµ›ç¬¬ä¸€åï¼ŒCCFæ¯”èµ›ç¬¬äºŒåï¼Œç§‘å¤§è®¯é£æ¯”èµ›ç¬¬ä¸‰åã€‚æ‹¥æœ‰å¤šé¡¹å‘æ˜ä¸“åˆ©ã€‚å¯¹æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ‹¥æœ‰è‡ªå·±ç‹¬åˆ°çš„è§è§£ã€‚æ›¾ç»è¾…å¯¼è¿‡è‹¥å¹²ä¸ªéè®¡ç®—æœºä¸“ä¸šçš„å­¦ç”Ÿè¿›å…¥åˆ°ç®—æ³•è¡Œä¸šå°±ä¸šã€‚å¸Œæœ›å’Œå¤§å®¶ä¸€èµ·æˆé•¿è¿›æ­¥ã€‚

â€ƒâ€ƒä»Šå¤©ç»™å¤§å®¶å¸¦æ¥çš„æ–‡ç« æ˜¯é©¾é©­AIç»˜ç”»ï¼šã€ŠAIé­”æ³•ç»˜ç”»ã€‹å¸¦ä½ ç§’å˜é¡¶çº§ç”»æ‰‹ï¼ï¼Œå¸Œæœ›èƒ½å¯¹å­¦ä¹ AIç»˜ç”»çš„åŒå­¦ä»¬æœ‰æ‰€å¸®åŠ©ã€‚



#### æ–‡ç« ç›®å½•
- - - - 


## 1. å‰è¨€

åŒå­¦ä»¬ï¼ŒæŠ–éŸ³ä¹°å®¶ç§€ä¸­çš„é‚£äº›æå…·è®¾è®¡æ„Ÿçš„ä½œå“æ˜¯æ€ä¹ˆæ¥çš„ï¼Ÿ

ä½ è¿˜åœ¨ç¾¡æ…•åˆ«äººçš„ç»˜ç”»å¤©èµ‹ï¼Œå¯¹ç€ç©ºç™½çš„ç”»çº¸å‘å‘†å—ï¼Ÿ

æœ‰æ²¡æœ‰æƒ³è¿‡å°†ç”Ÿç¡¬çš„ä»£ç å’ŒçµåŠ¨çš„è‰ºæœ¯å®Œç¾èåˆï¼Ÿ

æœ‰æ²¡æœ‰æ„Ÿåˆ°ç°ä»£æŠ€æœ¯å¸¦æ¥äº†æˆå‰§æ€§æ”¹å˜ï¼Œä»æ›¿ä»£é©¬è½¦çš„æ±½è½¦ï¼Œåˆ°å–ä»£ä¼ ç»Ÿå½±åƒçš„æ•°å­—æ‘„å½±ï¼Œå†åˆ°ä»Šå¤©çš„AIç¾æœ¯åˆ›ä½œï¼Ÿ

**æ˜¯çš„ï¼Œâ€œAIç»˜ç”»â€ï¼Œå®ƒæ¥äº†ã€‚**

AIç»˜ç”»ï¼Œå¦‚é›·è´¯è€³ï¼Œæ— å¤„ä¸åœ¨ã€‚

æ›¾å‡ ä½•æ—¶ï¼Œäººä»¬è®¤ä¸ºè‰ºæœ¯éœ€è¦äººå·¥åˆ›ä½œï¼Œéœ€è¦æ‰æƒ…ä¸çµæ„Ÿçš„ç¬é—´è¿¸å‘ã€‚

ç„¶è€Œï¼ŒæŠ€æœ¯çš„é«˜é€Ÿå‘å±•ï¼Œæ‰“ç ´äº†è¿™ç§è§‚å¿µï¼ŒAIç»˜ç”»æ­£åœ¨é€æ¸å´­éœ²å¤´è§’ï¼Œæˆä¸ºæ–°çš„ç”»å›å·¨æ“˜ã€‚

ä½†èŠä¸€èŠçœ‹ä¼¼é«˜å¤§ä¸Šçš„AIæŠ€æœ¯å¯èƒ½è®©ä½ è§‰å¾—é«˜ä¸å¯æ”€ã€‚é‚£å°±è·Ÿæˆ‘ä¸€èµ·ï¼Œ**ç”¨Stable DiffusionæŒ‘æˆ˜æ— é™å¯èƒ½ï¼Œä¸€çª¥AIç»˜ç”»çš„èŠ±èŠ±ä¸–ç•Œï¼**

<img src="https://img-blog.csdnimg.cn/img_convert/0b71cb00d011159b3f3e1f1d12fb8139.jpeg#pic_center" alt=""> â€ƒâ€ƒStable Diffusion WebUI ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œäººè„¸å›¾åƒé¢éƒ¨ç”»é¢ä¿®å¤æ¨¡å‹ CodeFormerçš„æ ¸å¿ƒä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```
import math
import torch
from torch import nn, Tensor
import torch.nn.functional as F
from typing import Optional

from modules.codeformer.vqgan_arch import VQAutoEncoder, ResBlock
from basicsr.utils.registry import ARCH_REGISTRY


class CodeFormer(VQAutoEncoder):
    def __init__(self, dim_embd=512, n_head=8, n_layers=9,
                codebook_size=1024, latent_size=256,
                connect_list=('32', '64', '128', '256'),
                fix_modules=('quantize', 'generator')):
        super(CodeFormer, self).__init__(512, 64, [1, 2, 2, 4, 4, 8], 'nearest',2, [16], codebook_size)

        if fix_modules is not None:
            for module in fix_modules:
                for param in getattr(self, module).parameters():
                    param.requires_grad = False

        self.connect_list = connect_list
        self.n_layers = n_layers
        self.dim_embd = dim_embd
        self.dim_mlp = dim_embd*2

        self.position_emb = nn.Parameter(torch.zeros(latent_size, self.dim_embd))
        self.feat_emb = nn.Linear(256, self.dim_embd)

        # transformer
        self.ft_layers = nn.Sequential(*[TransformerSALayer(embed_dim=dim_embd, nhead=n_head, dim_mlp=self.dim_mlp, dropout=0.0)
                                    for _ in range(self.n_layers)])

        # logits_predict head
        self.idx_pred_layer = nn.Sequential(
            nn.LayerNorm(dim_embd),
            nn.Linear(dim_embd, codebook_size, bias=False))

        self.channels = {<!-- -->
            '16': 512,
            '32': 256,
            '64': 256,
            '128': 128,
            '256': 128,
            '512': 64,
        }

        # after second residual block for &gt; 16, before attn layer for ==16
        self.fuse_encoder_block = {<!-- -->'512':2, '256':5, '128':8, '64':11, '32':14, '16':18}
        # after first residual block for &gt; 16, before attn layer for ==16
        self.fuse_generator_block = {<!-- -->'16':6, '32': 9, '64':12, '128':15, '256':18, '512':21}

        # fuse_convs_dict
        self.fuse_convs_dict = nn.ModuleDict()
        for f_size in self.connect_list:
            in_ch = self.channels[f_size]
            self.fuse_convs_dict[f_size] = Fuse_sft_block(in_ch, in_ch)

    def _init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
            if isinstance(module, nn.Linear) and module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, x, w=0, detach_16=True, code_only=False, adain=False):
        # ################### Encoder #####################
        enc_feat_dict = {<!-- -->}
        out_list = [self.fuse_encoder_block[f_size] for f_size in self.connect_list]
        for i, block in enumerate(self.encoder.blocks):
            x = block(x)
            if i in out_list:
                enc_feat_dict[str(x.shape[-1])] = x.clone()

        lq_feat = x
        # ################# Transformer ###################
        # quant_feat, codebook_loss, quant_stats = self.quantize(lq_feat)
        pos_emb = self.position_emb.unsqueeze(1).repeat(1,x.shape[0],1)
        # BCHW -&gt; BC(HW) -&gt; (HW)BC
        feat_emb = self.feat_emb(lq_feat.flatten(2).permute(2,0,1))
        query_emb = feat_emb
        # Transformer encoder
        for layer in self.ft_layers:
            query_emb = layer(query_emb, query_pos=pos_emb)

        # output logits
        logits = self.idx_pred_layer(query_emb) # (hw)bn
        logits = logits.permute(1,0,2) # (hw)bn -&gt; b(hw)n

        if code_only: # for training stage II
          # logits doesn't need softmax before cross_entropy loss
            return logits, lq_feat

        # ################# Quantization ###################
        # if self.training:
        #     quant_feat = torch.einsum('btn,nc-&gt;btc', [soft_one_hot, self.quantize.embedding.weight])
        #     # b(hw)c -&gt; bc(hw) -&gt; bchw
        #     quant_feat = quant_feat.permute(0,2,1).view(lq_feat.shape)
        # ------------
        soft_one_hot = F.softmax(logits, dim=2)
        _, top_idx = torch.topk(soft_one_hot, 1, dim=2)
        quant_feat = self.quantize.get_codebook_feat(top_idx, shape=[x.shape[0],16,16,256])
        # preserve gradients
        # quant_feat = lq_feat + (quant_feat - lq_feat).detach()

        if detach_16:
            quant_feat = quant_feat.detach() # for training stage III
        if adain:
            quant_feat = adaptive_instance_normalization(quant_feat, lq_feat)

        # ################## Generator ####################
        x = quant_feat
        fuse_list = [self.fuse_generator_block[f_size] for f_size in self.connect_list]

        for i, block in enumerate(self.generator.blocks):
            x = block(x)
            if i in fuse_list: # fuse after i-th block
                f_size = str(x.shape[-1])
                if w&gt;0:
                    x = self.fuse_convs_dict[f_size](enc_feat_dict[f_size].detach(), x, w)
        out = x
        # logits doesn't need softmax before cross_entropy loss
        return out, logits, lq_feat

```

## 2. ä¹¦ç±æ¨èã€ŠAIé­”æ³•ç»˜ç”»ï¼šç”¨Stable DiffusionæŒ‘æˆ˜æ— é™å¯èƒ½ã€‹

ä¿—è¯è¯´ï¼ŒçŸ¥è¯†å°±æ˜¯åŠ›é‡ï¼ŒæŒæ¡AIç»˜ç”»æŠ€æœ¯ï¼Œä½ ä¸ä»…å¯ä»¥åˆ›é€ å‡ºä»¤äººçœ¼èŠ±ç¼­ä¹±çš„è‰ºæœ¯ä½œå“ï¼Œè¿˜å¯èƒ½åœ¨ä¼—å¤šåŒè¡Œä¸­è„±é¢–è€Œå‡ºï¼Œèµ°ä¸ŠèŒä¸šç”Ÿæ¶¯çš„æ–°é«˜å³°ã€‚

çœ‹çœ‹é‚£äº›å› ä¸ºAIçš„åˆ°æ¥ï¼Œå¼€å§‹å¤§è§„æ¨¡è£å‘˜çš„å…¬å¸ï¼Œä½ æ˜¯å¦æ„è¯†åˆ°äº†è‡ªå·±è¯¥å­¦ä¹ AIç»˜ç”»çš„é‡è¦æ€§ï¼Ÿç­”æ¡ˆæ˜¯æ˜æ˜¾çš„ã€‚æŒæ¡æ½®æµï¼Œä¸»å®°æœªæ¥ï¼Œä½ å‡†å¤‡å¥½äº†å—ï¼Ÿ

è¿™é‡Œï¼Œå‘å¤§å®¶å¼ºçƒˆæ¨èä¸€æœ¬ä¹¦â€”â€”**ã€ŠAIé­”æ³•ç»˜ç”»ï¼šç”¨Stable DiffusionæŒ‘æˆ˜æ— é™å¯èƒ½ã€‹**ã€‚

<img src="https://img-blog.csdnimg.cn/img_convert/0abf6794829d5333488281adea7eb54b.png#pic_center" alt="">

**å®ƒæ˜¯ä¸€æœ¬ä»¥å®é™…æ“ä½œä¸ºå¯¼å‘çš„å…¥é—¨çº§AIç»˜ç”»å›¾ä¹¦ã€‚**

ä¹¦ä¸­è¯¦ç»†è®²è§£äº†åŸºäºStable Diffusionè¿›è¡ŒAIç»˜ç”»çš„å®Œæ•´å­¦ä¹ è·¯çº¿ï¼ŒåŒ…æ‹¬ç»˜ç”»æŠ€å·§ã€å›¾ç‰‡ç”Ÿæˆã€æç¤ºè¯ç¼–å†™ã€ControlNetæ’ä»¶ã€æ¨¡å‹è®­ç»ƒç­‰ç­‰ã€‚

å¦‚æœä½ æ˜¯é›¶åŸºç¡€ï¼Œæ— é¡»ææƒ§ï¼Œä¹¦ä¸­æ¶µç›–äº†ä¸°å¯Œçš„å®é™…æ“ä½œæ¡ˆä¾‹ï¼Œæ˜“æ‡‚æ˜“å­¦ï¼Œè½»æ¾å…¥é—¨ã€‚

å¦‚æœä½ å·²ç»æ­¥å…¥èŒåœºï¼Œé‚£ä¹ˆè¿™æœ¬ä¹¦ä¸­çš„ä¸°å¯ŒæŠ€æœ¯å†…å®¹ï¼Œç»å¯¹èƒ½è®©ä½ å¤§æœ‰æ”¶è·ã€‚

<img src="https://img-blog.csdnimg.cn/img_convert/581c99eede6f21bd5cc62b899f67c0e0.png#pic_center" alt="">

é‡è¦çš„æ˜¯ï¼Œæœ¬ä¹¦çš„å†…å®¹è¶…çº§å…¨é¢ã€‚æ— è®ºæ˜¯å§‹äºå…¥é—¨çš„åŸºç¡€ç†è®ºçŸ¥è¯†ï¼Œè¿˜æ˜¯No-Codeæ¨¡å‹è®­ç»ƒå¿…å¤‡çš„ControlNetæ’ä»¶çš„è¯¦è§£ï¼Œç”šè‡³æ˜¯AIç»˜ç”»å…¨æµç¨‹çš„æ“ä½œæ•™ç¨‹ï¼Œæœ¬ä¹¦ä¸€åº”ä¿±å…¨ï¼Œè®©ä½ æå‰é¢„çŸ¥å¹¶æŒæ¡AIç»˜ç”»çš„å…¨è²Œï¼Œ**å°†ä½ ä»èœé¸Ÿçº§åˆ«æå‡è‡³å¤§å¸ˆçº§**ã€‚

<img src="https://img-blog.csdnimg.cn/img_convert/5bb2bf7b99654af3fd77a899a606b7cd.png#pic_center" alt="">

æ­¤å¤–ï¼Œ**ä¹¦ä¸­åŒ…æ‹¬äº†ä¸€äº›éå¸¸å®ç”¨çš„å•†ä¸šè®¾è®¡æ¡ˆä¾‹ï¼Œ**å¦‚å®¶å…·æ•ˆæœå›¾ã€AIæ’ç”»ä¸æ’å›¾ã€AIå® ç‰©ã€åŸåˆ›IPè§’è‰²ã€è‡ªåª’ä½“è¿è¥ç­‰ï¼Œè¿™å°†ä¸ºä½ çš„åˆ›æ–°è®¾è®¡æ€è·¯æ·»ç –åŠ ç“¦ï¼

<img src="https://img-blog.csdnimg.cn/img_convert/aa38d905f85898c9967723611f0799ba.jpeg#pic_center" alt="">

<img src="https://img-blog.csdnimg.cn/img_convert/794ced291f829c40027d238f8c39e5de.jpeg#pic_center" alt="">

<img src="https://img-blog.csdnimg.cn/img_convert/58ca75bfed63870a4bcbd4dcd0ad9464.jpeg#pic_center" alt="">

<img src="https://img-blog.csdnimg.cn/img_convert/be7cf0185f92fbc841ec1c4cdf227b9e.jpeg#pic_center" alt="">

**â€¦â€¦**

**èµ„æºä¸°å¯Œï¼Œå†…å®¹è¯¦ç»†ï¼Œå„ç« èŠ‚çŸ¥è¯†ä½“ç³»å®Œå¤‡ï¼Œç¤ºä¾‹å½¢è±¡ç”ŸåŠ¨ï¼Œæ“ä½œæ­¥éª¤æ¸…æ™°æ˜äº†**ï¼Œè¿˜æœ‰è¯»è€…äº¤æµç¾¤å¯ä¸ä½œè€…äº’åŠ¨ï¼Œ**æœ¬ä¹¦ç®€ç›´æ˜¯ä½ å­¦ä¹ AIç»˜ç”»çš„ç»ä½³æ•™ç¨‹ï¼**

<img src="https://img-blog.csdnimg.cn/img_convert/a6f3fb9e5cf2fb6b4f7ad64175317101.png#pic_center" alt="">

æœ‹å‹ä»¬ï¼Œç”Ÿæ´»æ°¸è¿œå……æ»¡æ— é™å¯èƒ½ã€‚

ä½†æ˜¯ï¼Œå¦‚æœä½ æƒ³æŠŠæ¡æœªæ¥çš„æ½®æµï¼Œæ‰¬å¸†èµ·èˆªï¼Œé‚£ä¹ˆè®°å¾—è£…å¤‡è‡ªå·±ï¼Œå‹‡å¾€ç›´å‰ã€‚

å¿«æ¥ï¼å¼€å§‹ä½ çš„AIç»˜ç”»ä¹‹æ—…ï¼Œä¸æˆ‘ä¸€èµ·ï¼Œ**ç”¨ã€ŠAIé­”æ³•ç»˜ç”»ï¼Œç”¨stable DiffusionæŒ‘æˆ˜æ— é™å¯èƒ½ã€‹æ¢ç§˜AIç»˜ç”»çš„å¥¥å¦™ä¹‹å¤„ï¼Œ**åœ¨ä¸æ–­è¿­ä»£çš„æŠ€æœ¯é©±åŠ¨ä¸‹ï¼Œä¸€èµ·èµ°è¿›ç¥å¥‡çš„AIç»˜ç”»ä¸–ç•Œï¼Œä¸€èµ·æŒ‘æˆ˜æ— é™å¯èƒ½å§ï¼

<img src="https://img-blog.csdnimg.cn/img_convert/4ca77fbc8223e89661292c3a4312438b.png#pic_center" alt="">

<img src="https://img-blog.csdnimg.cn/img_convert/5a9c43db1c0924cd0f65996fdbd5cdc4.png#pic_center" alt="">

## 3. ç²‰ä¸ç¦åˆ©

â€ƒâ€ƒğŸæœ¬æ¬¡é€ä¹¦1~3æœ¬ã€å–å†³äºé˜…è¯»é‡ï¼Œé˜…è¯»é‡è¶Šå¤šï¼Œé€çš„è¶Šå¤šã€‘ğŸ‘ˆ â€ƒâ€ƒâŒ›ï¸æ´»åŠ¨æ—¶é—´ï¼šæˆªæ­¢åˆ°2024-1æœˆ13å· â€ƒâ€ƒâœ³ï¸å‚ä¸æ–¹å¼ï¼šå…³æ³¨åšä¸»+ä¸‰è¿ï¼ˆç‚¹èµã€æ”¶è—ã€è¯„è®ºï¼‰

## 4. è‡ªä¸»è´­ä¹°

â€ƒâ€ƒå°ä¼™ä¼´ä¹Ÿå¯ä»¥è®¿é—®é“¾æ¥è¿›è¡Œè‡ªä¸»è´­ä¹°å“¦~

â€ƒâ€ƒç›´è¾¾äº¬ä¸œè´­ä¹°é“¾æ¥ğŸ”—ï¼šè´­ä¹°åœ°å€
