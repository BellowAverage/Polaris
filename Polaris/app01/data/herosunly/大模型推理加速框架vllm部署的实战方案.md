
--- 
title:  大模型推理加速框架vllm部署的实战方案 
tags: []
categories: [] 

---
  大家好，我是herosunly。985院校硕士毕业，现担任算法研究员一职，热衷于机器学习算法研究与应用。曾获得阿里云天池比赛第一名，CCF比赛第二名，科大讯飞比赛第三名。拥有多项发明专利。对机器学习和深度学习拥有自己独到的见解。曾经辅导过若干个非计算机专业的学生进入到算法行业就业。希望和大家一起成长进步。

  本文主要介绍了大模型推理加速框架vllm部署的实战方案，希望对学习大语言模型的同学们有所帮助。 

#### 文章目录

  - 
  - 
  <li>
   <ul>
    - 
    - 
   


## 1. 前言

  vLLM是一个基于Python的LLM（大语言模型）推理和服务框架，它的主要优势包括简单易用和性能高效。 <img src="https://img-blog.csdnimg.cn/c8b6c44bf2b146899f5a0b0e681ee927.png#pic_center" alt="在这里插入图片描述">

具体优势如下：

 - 超强的服务吞吐量
 - 使用PagedAttention高效管理注意力key和value
 - 连续批处理传入请求
 - 对CUDA核心进行了优化

vLLM灵活易用&amp;#
