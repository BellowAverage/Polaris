
--- 
title:  白话 Transformer 原理-以 BERT 模型为例 
tags: []
categories: [] 

---
## 白话 Transformer 原理-以 BERT 模型为例

## 第一部分：引入

### 1-向量

在数字化时代，数学运算最小单位通常是自然数字，但在 AI 时代，这个最小单元变成了向量，**这是数字化时代计算和智能化时代最重要的差别之一**。

举个例子：银行在放款前，需要评估一个人的信用度；对于用户而言，可以简单地用一个向量来表示其信用度，如下所示：

>  
 [ID，性别，年龄，学历，地区，年收入，存款，交易额，违约] 


向量是一组数据的集合，可以想象成在一个超高维度空间里的一个点。一个具体的信用度向量，就是在 9 个特征组成的高维空间的一个点。数据在高维空间将展现更多的数学性质，容易让我们抓住更多隐藏的规律。在 AI 领域，向量的维度可达数千维、甚至数万维。以 BERT-Base 中文模型为例，每个汉字都被表示为 768 维的向量。

如下图所示，从人们的认知来看，超过 3 维空间便很难直观的理解了。在 AI 时代，数据的维度远远超过 3 维，我们不能再以传统的思维来看待问题。 <img src="https://img-blog.csdnimg.cn/direct/520998212f6b4551bb6e85c2757257f1.jpeg#pic_center" alt="在这里插入图片描述" width="600" height="300">  

### 
