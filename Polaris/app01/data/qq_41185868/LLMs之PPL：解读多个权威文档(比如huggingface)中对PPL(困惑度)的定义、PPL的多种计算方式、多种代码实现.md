
--- 
title:  LLMs之PPL：解读多个权威文档(比如huggingface)中对PPL(困惑度)的定义、PPL的多种计算方式、多种代码实现 
tags: []
categories: [] 

---
LLMs之PPL：解读多个权威文档(比如huggingface)中对PPL(困惑度)的定义、PPL的多种计算方式、多种代码实现



>  
 **<strong>导读**</strong>：这篇文章主要分析了固定长度模型计算困惑度(perplexity)的问题。首先给出了困惑度的定义。困惑度反映了模型预测序列每个token的能力，是经常用于评估语言模型的一个重要指标。对于顺序语言模型，可以将序列分解为条件概率，然后计算困惑度。但对于固定长度模型，如GPT系列，由于模型有最大输入长度限制，无法得到条件上下文所有token的概率。文章提出了两种计算困惑度的方法： 
 T1、将序列切分为不相交的块，分别计算每个块的负对数似然，然后相加。这种方法计算简单，但近似不准，通常得到的困惑度值会更高。 
 T2、使用滑动窗口策略。将上下文窗口按一定步长滑动计算每个token的条件概率。这种方法近似真实分解，通常能得到更佳的困惑度值。但计算代价较高。 
 文中以GPT-

