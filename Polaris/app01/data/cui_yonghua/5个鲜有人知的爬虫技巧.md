
--- 
title:  5个鲜有人知的爬虫技巧 
tags: []
categories: [] 

---
几点鲜有人知的爬虫技巧

### 技巧一 换个角度，解锁新姿势

在爬取某些 web 网站的时候，被各种反爬弄得哭天喊地。

什么几把 css 字体加密，什么几把 js 的 MD5 等，各种乱七八糟的加密，什么各种飞的验证

这时候，就别太执着于 web PC 端嘛，咱们`去看看人家的移动端，看看人家的 H5`，可能你会在移动端发现惊喜，数据都是一样的数据，冤冤相报何时了。

### 技巧二 频率不要太快

在爬取的时候，要学会停顿，克制一点。

该 sleep 就 sleep， IP 才不会频繁被封

### 技巧三 善用他人的 UA

如果你去看别人网站的 robots.txt，你就会看到别人的声明：声明什么东西是可以爬取，什么东西是不允许被爬的

但你常常忽略了一个东西，人家声明了希望给什么搜索引擎爬，比如这个 <img src="https://img-blog.csdnimg.cn/9dfd7aa576e24e91a95def53b54a804b.png" alt="在这里插入图片描述">

看到没，这是别人定义的 robots，值得注意的是：尚亮亮的 User-agent

那么当你在 Python 构造 Header 的时候，`User-agent 就直接指定他们 robots 定义的就好`了啊

比如 百度的UA，google的UA，360的UA。你再去爬取看看，那是一个友好啊

### 技巧四 快速生成 Header

每一次你在复制 request header 的时候，是不是有一大串有的没的，又不得不复制过来，在你的 Python 中使用，每次操作很麻烦是不？

那你可以自己写一个方法：`参数就是你复制的 header 字符串，然后生成 header 的字典格式`

### 技巧五 找到对方的 sitemap.xml

有时候你想爬取整个网站的url，怎么办呢？

不是去首页一个一个抓，你应该找到对方的 sitemap.xml，因为网站一般希望 Google 或者百度快点收录他们的网站，所以他们会把自己的网站的 url 生成 sitemap 提交，这个时候 `sitemap 就包含了这个网站所有可爬取的 url`。

sitemap一般在网站的根目录下，可以在他们的 robots.txt 看看他们指定的位置，比如猫眼电影的sitemap： <img src="https://img-blog.csdnimg.cn/af468c66903e41599bb4163540db9157.png" alt="在这里插入图片描述">

从而获取 sitemap 再去请求里面的 url 即可！
