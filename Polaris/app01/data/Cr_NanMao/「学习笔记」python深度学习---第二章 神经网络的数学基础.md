
--- 
title:  「学习笔记」python深度学习---第二章 神经网络的数学基础 
tags: []
categories: [] 

---
### 2.1 初识神经网络

#### 一、MNIST数据集

下载网址： 

****

包含60000张训练图像和10000张测试图像

<img alt="" height="109" src="https://img-blog.csdnimg.cn/9a34d52dbf15469fa041e9c4763b17c0.png" width="453">



#### 二、 关于类和标签的说明

类别：在机器学习中，分类问题中的某个类别叫作类（class）。

样本：数据点叫作样本（sample）。

标签：某个样本对应的类叫做标签（label）。



#### 三、

1. keras以TensorFlow和Theano作为后端封装，是一个专门用于深度学习的python模块。 2. 包含了全连接层，卷积层，池化层，循环层，嵌入层等等等，常见的深度学习模型。 3. 包含用于定义损失函数的Losses，用于训练模型的Optimizers，评估模型的Metrics，定义激活函数的Activations，防止过拟合的Regularizers等功能。 4. keras不需要了解太多理论知识，可以直接使用模型，新手入门十分友好。 5. 训练集：train_images和train_labels组成了训练集，模型将从这些数据中进行学习。

6. 测试集：在测试集即test_images和test_labels上对模型进行测试。

7. 神经网络的核心组件是层（layer），它是一种数据处理模块，你可以将它看成数据过滤器。进去一些数据，出来的数据变得更加有用。

8. 深度学习模型就像是数据处理的筛子，包含一系列越来越精细的数据过滤器（即层）。

9. 密集连接（全连接），softmax层。

10. 想要训练网络，我们还需要选择编译步骤的三个参数：

（1）损失函数（loss function）：网络如何衡量在训练数据上的性能，即网络如何朝着正确的方向前进。

（2）优化器（optimizer）：基于训练数据和损失函数来更新网络的机制。

（3）在训练和测试过程中需要监控的指标(metric)：正确分类的图像所占的比例。



11. 拟合fit：在keras中这一步是通过调用网络的fit方法来完成的---我们在训练数据上拟合模型。

12. 过拟合overfit：训练精度和测试精度之间的这种差距是过拟合造成的。



#### 四、神经网络的数据表示

1. 张量（数据容器）：一般来说，当前所有机器学习系统都使用张量作为基本数据结构。张量这一概念的核心在于，它是一个数据容器。它包含的数据几乎总是数值数据，因此它是数字的容器。张量是矩阵向任意维度的推广（张量的维度（dimension）通常叫作轴（axis））。

2. 标量（0D张量）：仅包含一个数字的张量叫作标量（scalar，也叫标量张量、零维张量、0D张量）。

3. 向量（1D张量）：数字组成的数组叫作向量（vector）或一维张量（1D张量）。一维张量只有一个轴。

4. 矩阵（2D张量）：向量组成的数组叫作矩阵（matrix）或二维张量（2D张量）。矩阵有2个轴（通常叫作行和列）。

5. 3D张量与更高维张量：将多个矩阵组合成一个新的数组，可以得到一个3D张量，你可以将其直观地理解为数字组成的立方体。将多个3D张量组合成一个数组，可以创建一个4D张量，以此类推。深度学习处理的一般是0D到4D的张量，但处理视频数据时可能会遇到5D张量。

6. 关键属性

（1）轴的个数（阶）：例如，3D张量有3个轴，矩阵有2个轴。这在Numpy等Python库中也叫张量的ndim。

（2）形状：这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。例如，前面矩阵示例的形状为（3，5），3D张量示例的形状为（3，3，5）。向量的形状只包含一个元素，比如（5，），而标量的形状为空，即（）。

（3）数据类型（在Python库中通常叫作dtype）：这是张量中所包含数据的类型，例如，张量的类型可以时float32、unit8、float64等。在极少数情况下，你可能会遇到字符（char）张量。注意，Numpy（以及大多数其他库）中不存在字符串张量，因为张量存储在预先分配的连续内存段中，而字符串的长度是可变的，无法用这种方式存储。

7. 张量切片（tensor slicing）：选择张量的特定元素叫作张量切片。

8. 样本轴（样本维度）：通常来说，深度学习中所有数据张量的第一个轴（0轴，因为索引从0开始）都是样本轴（sample axis）。

9. 批量轴（批量维度）和特征轴：

        batch = train_images[:128]

        batch = train_images[128:256]

        ......

        batch = train_images[128*n:128*(n+1)]

对于这种批量张量，第一个轴（0轴）叫作批量轴（batch axis）或批量维度（batch dimension）。第二个轴是特征轴。

10. 现实世界中的数据张量

（1）向量数据：2D张量，形状为（samples，features）。

（2）时间序列数据或序列数据：3D张量，形状为（samples，timesteps，features）。

（3）图像：4D张量，形状为（samples，height，width，channels）或（samples，channels，height，width）。

（4）视频：5D张量，形状为（samples，frames，height，width，channels）或者（samples，frames，channels，height，width）。

11. 时间序列数据或序列数据：当时间（或序列顺序）对于数据很重要时，应该将数据存储在带有时间轴的3D张量中。每个样本可以被编码为一个向量序列（即2D张量），因此一个数据批量就被编码为一个3D张量。根据惯例，时间轴始终是第2个轴（索引为1的轴）。



#### 五、神经网络的“齿轮”：张量运算

1. 张量运算：深度神经网络学到的所有变换也都可以简化为数值数据张量上的一些张量计算（tensor operation），例如加上张量、乘以张量等。

（1）加法运算；（2）乘法运算；（3）缩并运算；（4）内积运算；

2. 逐元素运算：relu运算和加法都是逐元素（element-wise）的运算，即该运算独立地应用于张量中的每个元素，也就是说，这些运算非常适合大规模并行实现。

3. 向量化实现：具体解释➡️

4. 向量处理器：向量处理器，也称为阵列处理器​​​​​​​，能够同步进行综合数据的运算操作；而大多数的 CPU 属于纯量处理器，只能一次处理一个要素。

5. 广播：广播（broadcast）包含以下两步：

（1）向较小的张量添加轴（叫作广播轴），使其ndim与较小的张量相同。

（2）将较小的张量沿着新轴重复，使其形状与较大的张量相同。

6. 张量点积：

        点积运算，也叫张量积（tensor product，不要与逐元素的乘积弄混），是最常见也最有用的张量运算。与逐元素的运算不同，它将输入张量的元素合并在一起。

        在Numpy、Keras、Theano和TensorFlow中，都是用*实现逐元素乘积。TensorFlow中的点积使用了不同的语法，但在Numpy和Keras中，都是用标准的dot运算符来实现点积。

        注意：两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。

7. 张量变形：张量变形（tensor reshaping）是指改变张量的行和列，以得到想要的形状。变形后的张量的元素总个数与初始张量相同。

8. 转置：经常遇到的一种特殊的张量变形是转置（transposition）。对矩阵做转置是指将行和列互换，使x[i , :]变为x[: , i]。



#### 五、神经网络的“引擎”： 基于梯度的优化

1. output = relu(dot(w , input) + b)

        在这个表达式中，w和b都是张量，均为该层的属性。它们被称为该层的权重（weight）或可训练参数（trainable parameter），分别对应kernel和bias属性。这些权重包含网络从观察训练数据中学到的信息。

        一开始，这些权重矩阵取较小的随机值，这一步叫作随机初始化（random initialization）。当然，w和b都是随机的，output = relu(dot(w , input) + b)肯定不会得到任何有用的表示。虽然得到的表示是没有意义的，但这是一个起点。下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫做训练，也就是机器学习中的学习。

2. 训练循环（training loop）：

训练循环按顺序重复执行以下任务：
- 发送一批输入值，通过模型生成输出值- 通过比较输出值与输出(标签)，来计算损失值- 使用梯度带(GradientTape)找到梯度值- 使用这些梯度优化变量
3. 向前传播算法、反向传播算法：在这篇文章中对向前传播算法和反向传播算法有很详细的解说。（但是文章中有个别错误，可以配合评论的纠正看）

4. 可微：是“可以被求导”的意思。

5. 梯度（gradient）：是张量运算的导数。它是导数这一概念向多元函数导数的推广。多元函数是以张量作为输入的函数。

6. 随机梯度下降：

        给定一个可微函数，理论上可以用解析法找到它的最小值：函数的最小值是导数为0的点，因此你只需找到所有导数为0的点，然后计算函数在其中哪个点具有最小值。

        将这一方法应用于神经网络，就是用解析法求出最小损失函数对应的所有权重值。可以通过对方程gradient(f)(W) = 0求解W来实现这一方法。这是包含N个变量的多项式方程，其中N是网络中系数的个数。N=2或N=3时可以对这样的方程求解，但对于实际的神经网络是无法求解的，因为参数的个数不会少于几千个，而且经常有上千万个。

7. 小批量随机梯度下降：

(1) 抽取训练样本 x 和对应目标 y 组成的数据批量。

(2) 在 x 上运行网络，得到预测值 y_pred。

(3) 计算网络在这批数据上的损失，用于衡量 y_pred 和 y 之间的距离。

(4) 计算损失相对于网络参数的梯度［一次反向传播（ backward pass）］。

(5) 将参数沿着梯度的反方向移动一点，比如 W -= step * gradient，从而使这批数据上的损失减小一点。

        以上描述的方法叫作小批量随机梯度下降（mini-batch stochastic gradient descent，又称为小批量 SGD）。术语随机（stochastic）是指每批数据都是随机抽取的（stochastic 是 random在科学上的同义词）。

8. 真SGD：小批量SGD算法的一个变体是每次迭代时只抽取一个样本和目标，而不是抽取一批数据。这叫作真SGD（有别于小批量SGD）。

9. 批量SGD：每一次迭代都在所有数据上运行，这叫作批量SGD。

10. 优化方法或优化器：SGD还有多种变体，其区别在于计算下一次权重更新时还要考虑上一次权重更新，而不是仅仅考虑当前梯度值，比如带动量的SGD、Adagrad、RMSProp等变体。这些变体被称为优化方法（optimizatin method）或优化器（optimizer）。

11. 动量：在许多变体中都有应用。动量解决了SGD的两个问题：收敛速度和局部极小点。

### 本章小结

 1. 学习是指找到一组模型参数，使得在给定的训练数据样本和对应目标值上的损失函数最小化。

2. 学习的过程：随机选取包含数据样本及其目标值的批量，并计算批量损失相对于网络参数的梯度。随后将网络参数沿着梯度的反方向稍稍移动（移动距离由学习率指定）。

3.整个学习过程之所以能够实现，是因为神经网络时一系列可微分的张量运算，因此可以利用求导的链式法则来得到梯度函数，这个函数将当前参数和当前数据批量映射为一个梯度值。

4. 损失是在训练过程中需要最小化的量，因此，它应该能够衡量当前任务是否已成功解决。

5. 优化器是使用损失梯度更新参数的具体方式，比如RMSProp优化器、带动量的随机梯度下降（SGD）等。






