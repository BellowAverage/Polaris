
--- 
title:  CH01 统计学习方法概论 
tags: []
categories: [] 

---
## CH01 统计学习方法概论



#### 文章目录
- - <ul><li>- <ul><li>- - - <ul><li>- - - - - 


### 前言

#### 章节目录
1. 统计学习<li>监督学习 
  <ol>1. 基本概念1. 问题的形式化1. 模型1. 策略1. 算法1. 训练误差与测试误差1. 过拟合与模型选择1. 正则化1. 交叉验证1. 泛化误差1. 泛化误差上界
#### 导读
<li> 直接看目录结构，会感觉有点乱，就层级结构来讲感觉并不整齐。可以看本章概要部分，摘录几点，希望对理解本章内容编排有帮助： 
  <blockquote> 
   - 统计学习三要素对理解统计学习方法起到提纲挈领的作用- 本书主要讨论**监督学习**- 分类问题、标注问题和回归问题都是监督学习的重要问题- 本书中介绍的统计学习方法包括…。这些方法是主要的分类、标注以及回归方法。他们又可归类为生成方法与判别方法。 
  </blockquote> </li>-  本章最后的三个部分，这三个问题可以对比着看，如果暂时没有概念，略过也可以，回头对各个算法有了感觉回头再看这里。 这三部分怎么对比，三部分都有个图来说明，仔细看下差异，本文后面会对此展开。 -  关于损失函数，风险函数与目标函数注意体会差异 <li> 后面插点从深度学习角度拿到的点 
  <ul>- 关于机器学习三要素, 复旦大学邱锡鹏教授也有解读[^2]: 模型, 学习准则, 优化算法. 这个定义比较接近代码. 以Tensorflow为例. 通常会定义一个网络(模型), 定义Loss(学习准则), 定义优化算法(Optimizer), 然后开Session, 不停的把数据带入用Opitmizer去最小化Loss.- Losses, Metrics, 在Keras里面划分了两个模块, 解释是Losses是BP过程用到的, 而Metrics实际和损失函数类似, 用来评价模型的性能, 但是不参与反向传播. 从源码也能看到, Metrics里面import了很多Loss算法
书中例子1.1可以参考PRML中对应的表述， 更详细些。

### 实现统计学习方法的步骤

统计学习方法三要素:模型,策略,算法.

>  
 - 得到一个有限的训练数据集合- 确定包含所有可能的模型的**假设空间**, 即学习模型的集合.- 确定模型选择的准则, 即学习的**策略**- 实现求解最优模型的算法, 即学习的**算法**- 通过学习方法选择最优的模型- 利用学习的最优模型对新数据进行预测或分析. 


### 统计学习方法三要素

#### 模型

##### 模型是什么?

在监督学习过程中, 模型就是所要学习的**条件概率分布**或者**决策函数**.

注意书中的这部分描述，整理了一下到表格里： (1) 
     
      
       
       
         F 
        
        
        
          = 
         
        
          { 
         
         
         
           f 
          
         
           θ 
          
         
        
          ∣ 
         
        
          Y 
         
        
          = 
         
         
         
           f 
          
         
           θ 
          
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          , 
         
        
          θ 
         
        
          ∈ 
         
         
         
           R 
          
          
           
            
           
             n 
            
           
          
            } 
           
          
         
        
       
      
        \rm F\it =\{f_{\theta} |Y=f_{\theta}(x), \theta \in \bf R \it ^n\} 
       
      
    F={<!-- -->fθ​∣Y=fθ​(x),θ∈Rn} (2)  
     
      
       
       
         F 
        
        
        
          = 
         
        
          { 
         
        
          P 
         
        
          ∣ 
         
         
         
           P 
          
         
           θ 
          
         
        
          ( 
         
        
          Y 
         
        
          ∣ 
         
        
          X 
         
        
          ) 
         
        
          , 
         
        
          θ 
         
        
          ∈ 
         
         
         
           R 
          
          
           
            
           
             n 
            
           
          
            } 
           
          
         
        
       
      
        \rm F\it =\{P|P_{\theta}(Y|X),\theta\in \bf R \it ^n\} 
       
      
    F={<!-- -->P∣Pθ​(Y∣X),θ∈Rn}

||假设空间                                                  F                                          \rm F                           F|输入空间                                                  X                                          \rm X                           X|输出空间                                                  Y                                          \rm Y                           Y|参数空间
|------
|决策函数|(1)|变量|变量|                                                  R                                                           n                                                      \bf R\it ^n                           Rn
|条件概率分布|(2)|随机变量|随机变量|                                                  R                                                           n                                                      \bf R\it ^n                           Rn

书中描述的时候，有提到**条件概率分布族**，这个留一下，后面会提到确认逻辑斯谛分布属于指数分布族。

#### 策略

##### 损失函数与风险函数

>  
 **损失函数**度量模型**一次预测**的好坏，**风险函数**度量**平均意义**下模型预测的好坏。 

<li> <p>损失函数(loss function)或代价函数(cost function) 损失函数定义为给定输入 
       
        
         
         
           X 
          
         
        
          X 
         
        
      X的<strong>预测值 
        
         
          
          
            f 
           
          
            ( 
           
          
            X 
           
          
            ) 
           
          
         
           f(X) 
          
         
       f(X)**和**真实值 
        
         
          
          
            Y 
           
          
         
           Y 
          
         
       Y**之间的**非负实值</strong>函数, 记作 
       
        
         
         
           L 
          
         
           ( 
          
         
           Y 
          
         
           , 
          
         
           f 
          
         
           ( 
          
         
           X 
          
         
           ) 
          
         
           ) 
          
         
        
          L(Y,f(X)) 
         
        
      L(Y,f(X))</p> </li><li> <p>风险函数(risk function)或期望损失(expected loss) 这个和模型的泛化误差的形式是一样的  
       
        
         
          
          
            R 
           
           
           
             e 
            
           
             x 
            
           
             p 
            
           
          
         
           ( 
          
         
           f 
          
         
           ) 
          
         
           = 
          
          
          
            E 
           
          
            p 
           
          
         
           [ 
          
         
           L 
          
         
           ( 
          
         
           Y 
          
         
           , 
          
         
           f 
          
         
           ( 
          
         
           X 
          
         
           ) 
          
         
           ) 
          
         
           ] 
          
         
           = 
          
          
          
            ∫ 
           
           
           
             X 
            
           
             × 
            
           
             Y 
            
           
          
         
           L 
          
         
           ( 
          
         
           y 
          
         
           , 
          
         
           f 
          
         
           ( 
          
         
           x 
          
         
           ) 
          
         
           ) 
          
         
           P 
          
         
           ( 
          
         
           x 
          
         
           , 
          
         
           y 
          
         
           ) 
         &amp;ThinSpace; 
         
           d 
          
         
           x 
          
         
           d 
          
         
           y 
          
         
        
          R_{exp}(f)=E_p[L(Y, f(X))]=\int_{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\, {\rm d}x{\rm d}y 
         
        
      Rexp​(f)=Ep​[L(Y,f(X))]=∫X×Y​L(y,f(x))P(x,y)dxdy 模型 
       
        
         
         
           f 
          
         
           ( 
          
         
           X 
          
         
           ) 
          
         
        
          f(X) 
         
        
      f(X)关于联合分布 
       
        
         
         
           P 
          
         
           ( 
          
         
           X 
          
         
           , 
          
         
           Y 
          
         
           ) 
          
         
        
          P(X,Y) 
         
        
      P(X,Y)的**平均意义下的**损失(**期望**损失), 但是因为 
       
        
         
         
           P 
          
         
           ( 
          
         
           X 
          
         
           , 
          
         
           Y 
          
         
           ) 
          
         
        
          P(X,Y) 
         
        
      P(X,Y)是未知的, 所以前面的用词是**期望**, 以及**平均意义下的**.</p> <p>这个表示其实就是损失的均值, 反映了对整个数据的预测效果的好坏,  
       
        
         
         
           P 
          
         
           ( 
          
         
           x 
          
         
           , 
          
         
           y 
          
         
           ) 
          
         
        
          P(x,y) 
         
        
      P(x,y)转换成 
       
        
         
          
           
           
             ν 
            
           
             ( 
            
           
             X 
            
           
             = 
            
           
             x 
            
           
             , 
            
           
             Y 
            
           
             = 
            
           
             y 
            
           
             ) 
            
           
          
            N 
           
          
         
        
          \frac {\nu(X=x, Y=y)}{N} 
         
        
      Nν(X=x,Y=y)​更容易直观理解, 真实的数据N是无穷的.</p> </li><li> <p>**经验风险**(empirical risk)或**经验损失**(empirical loss)  
       
        
         
          
          
            R 
           
           
           
             e 
            
           
             m 
            
           
             p 
            
           
          
         
           ( 
          
         
           f 
          
         
           ) 
          
         
           = 
          
          
          
            1 
           
          
            N 
           
          
          
          
            ∑ 
           
           
           
             i 
            
           
             = 
            
           
             1 
            
           
          
            N 
           
          
         
           L 
          
         
           ( 
          
          
          
            y 
           
          
            i 
           
          
         
           , 
          
         
           f 
          
         
           ( 
          
          
          
            x 
           
          
            i 
           
          
         
           ) 
          
         
           ) 
          
         
        
          R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i)) 
         
        
      Remp​(f)=N1​∑i=1N​L(yi​,f(xi​)) 模型 
       
        
         
         
           f 
          
         
        
          f 
         
        
      f关于**训练样本集**的平均损失 根据大数定律, 当样本容量N趋于无穷大时, 经验风险趋于期望风险</p> </li><li> <p>**结构风险**(structural risk)  
       
        
         
          
          
            R 
           
           
           
             s 
            
           
             r 
            
           
             m 
            
           
          
         
           ( 
          
         
           f 
          
         
           ) 
          
         
           = 
          
          
          
            1 
           
          
            N 
           
          
          
          
            ∑ 
           
           
           
             i 
            
           
             = 
            
           
             1 
            
           
          
            N 
           
          
         
           L 
          
         
           ( 
          
          
          
            y 
           
          
            i 
           
          
         
           , 
          
         
           f 
          
         
           ( 
          
          
          
            x 
           
          
            i 
           
          
         
           ) 
          
         
           ) 
          
         
           + 
          
         
           λ 
          
         
           J 
          
         
           ( 
          
         
           f 
          
         
           ) 
          
         
        
          R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f) 
         
        
      Rsrm​(f)=N1​∑i=1N​L(yi​,f(xi​))+λJ(f)  
       
        
         
         
           J 
          
         
           ( 
          
         
           f 
          
         
           ) 
          
         
        
          J(f) 
         
        
      J(f)为模型复杂度,  
       
        
         
         
           λ 
          
         
           ⩾ 
          
         
           0 
          
         
        
          \lambda \geqslant 0 
         
        
      λ⩾0是系数, 用以权衡经验风险和模型复杂度.</p> </li>
##### 常用损失函数

损失函数数值越小，模型就越好

 
     
      
       
       
         L 
        
       
         ( 
        
       
         Y 
        
       
         , 
        
       
         f 
        
       
         ( 
        
       
         X 
        
       
         ) 
        
       
         ) 
        
       
      
        L(Y,f(X)) 
       
      
    L(Y,f(X))
<li>0-1损失  
      
       
        
        
          L 
         
        
          = 
         
         
         
           { 
          
          
           
            
             
              
              
                1 
               
              
                , 
               
              
                Y 
               
              
                ≠ 
               
              
                f 
               
              
                ( 
               
              
                X 
               
              
                ) 
               
              
             
            
           
           
            
             
              
              
                0 
               
              
                , 
               
              
                Y 
               
              
                = 
               
              
                f 
               
              
                ( 
               
              
                X 
               
              
                ) 
               
              
             
            
           
          
         
        
       
         L=\begin{cases}1, Y \neq f(X) \\0, Y=f(X) \end{cases} 
        
       
     L={<!-- -->1,Y̸​=f(X)0,Y=f(X)​</li><li>平方损失  
      
       
        
        
          L 
         
        
          = 
         
        
          ( 
         
        
          Y 
         
        
          − 
         
        
          f 
         
        
          ( 
         
        
          X 
         
        
          ) 
         
         
         
           ) 
          
         
           2 
          
         
        
       
         L=(Y-f(X))^2 
        
       
     L=(Y−f(X))2</li><li>绝对损失  
      
       
        
        
          L 
         
        
          = 
         
        
          ∣ 
         
        
          Y 
         
        
          − 
         
        
          f 
         
        
          ( 
         
        
          X 
         
        
          ) 
         
        
          ∣ 
         
        
       
         L=|Y-f(X)| 
        
       
     L=∣Y−f(X)∣</li>
 
     
      
       
       
         L 
        
       
         ( 
        
       
         Y 
        
       
         , 
        
       
         P 
        
       
         ( 
        
       
         Y 
        
       
         ∣ 
        
       
         X 
        
       
         ) 
        
       
         ) 
        
       
      
        L(Y,P(Y|X)) 
       
      
    L(Y,P(Y∣X))
<li>对数损失 这里 
      
       
        
        
          P 
         
        
          ( 
         
        
          Y 
         
        
          ∣ 
         
        
          X 
         
        
          ) 
         
        
          ⩽ 
         
        
          1 
         
        
       
         P(Y|X)\leqslant 1 
        
       
     P(Y∣X)⩽1，对应的对数是负值，所以对数损失中包含一个负号，为什么不是绝对值？因为肯定是负的。  
      
       
        
        
          L 
         
        
          = 
         
        
          − 
         
        
          log 
         
        
          ⁡ 
         
        
          P 
         
        
          ( 
         
        
          Y 
         
        
          ∣ 
         
        
          X 
         
        
          ) 
         
        
       
         L=-\log P(Y|X) 
        
       
     L=−logP(Y∣X)</li>
##### ERM与SRM

经验风险最小化(ERM)与结构风险最小化(SRM)
1. **极大似然估计**是经验风险最小化的一个例子. 当模型是条件概率分布, 损失函数是对数损失函数时, 经验风险最小化等价于极大似然估计.1. **贝叶斯估计**中的**最大后验概率估计**是结构风险最小化的一个例子. 当模型是条件概率分布, 损失函数是对数损失函数, **模型复杂度由模型的先验概率表示**时, 结构风险最小化等价于最大后验概率估计.
### 模型选择
1. 正则化 模型选择的典型方法是正则化<li>交叉验证 另一种常用的模型选择方法是交叉验证 
  1. 简单1. S折(K折, K-Fold)[^1]1. 留一法 </li>
### 泛化能力
-  现实中采用最多的方法是通过测试误差来评价学习方法的泛化能力 -  统计学习理论试图从理论上对学习方法的泛化能力进行分析 <li> 学习方法的泛化能力往往是通过研究泛化误差的**概率上界**进行的, 简称为泛化误差上界(generalization error bound) <p>这本书里面讨论的不多，在里面有讨论提升方法的误差分析, 提到 
       
        
         
         
           A 
          
         
           d 
          
         
           a 
          
         
           B 
          
         
           o 
          
         
           o 
          
         
           s 
          
         
           t 
          
         
        
          AdaBoost 
         
        
      AdaBoost不需要知道下界 
       
        
         
         
           γ 
          
         
        
          \gamma 
         
        
      γ。在中讨论算法的收敛性的时候有提到误分类次数的上界.</p> </li>
注意泛化误差的定义，书中有说**事实上，泛化误差就是所学习到的模型的期望风险**

### 生成模型与判别模型

**监督学习方法**可分为**生成方法**(generative approach)与**判别方法**(discriminative approach)

#### 生成方法

generative approach
<li>可以还原出**联合概率分布** 
      
       
        
        
          P 
         
        
          ( 
         
        
          X 
         
        
          , 
         
        
          Y 
         
        
          ) 
         
        
       
         P(X,Y) 
        
       
     P(X,Y)</li>- 收敛速度快, 当样本容量增加时, 学到的模型可以更快收敛到真实模型- 当存在隐变量时仍可以用
#### 判别方法

discriminative approach
<li>直接学习**条件概率** 
      
       
        
        
          P 
         
        
          ( 
         
        
          Y 
         
        
          ∣ 
         
        
          X 
         
        
          ) 
         
        
       
         P(Y|X) 
        
       
     P(Y∣X)或者**决策函数** 
      
       
        
        
          f 
         
        
          ( 
         
        
          X 
         
        
          ) 
         
        
       
         f(X) 
        
       
     f(X)</li>- 直接面对预测, 往往学习准确率更高- 可以对数据进行各种程度的抽象, 定义特征并使用特征, 可以简化学习问题
### 分类问题、标注问题、回归问题

Classification, Tagging, Regression
<li>图1.4和图1.5除了分类系统和标注系统的差异外，没看到其他差异，但实际上这两幅图中对应的输入数据有差异，序列数据的 
      
       
        
         
         
           x 
          
         
           i 
          
         
        
          = 
         
        
          ( 
         
         
         
           x 
          
         
           i 
          
          
          
            ( 
           
          
            1 
           
          
            ) 
           
          
         
        
          , 
         
         
         
           x 
          
         
           i 
          
          
          
            ( 
           
          
            2 
           
          
            ) 
           
          
         
        
          , 
         
        
          … 
         
        
          , 
         
         
         
           x 
          
         
           i 
          
          
          
            ( 
           
          
            n 
           
          
            ) 
           
          
         
         
         
           ) 
          
         
           T 
          
         
        
       
         x_i = (x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T 
        
       
     xi​=(xi(1)​,xi(2)​,…,xi(n)​)T对应了</li><li>图1.5和图1.6，回归问题的产出为 
      
       
        
        
          Y 
         
        
          = 
         
         
         
           f 
          
         
           ^ 
          
         
        
          ( 
         
        
          X 
         
        
          ) 
         
        
       
         Y=\hat f(X) 
        
       
     Y=f^​(X)</li>
### 参考

参考文献都是大部头，ESL，PRML在列
1.   1.  NNDL 