
--- 
title:  方差、偏差、噪声总结 
tags: []
categories: [] 

---
#### 偏差、方差、噪声

>  
 偏差：度量了模型的期望预测和真实结果的偏离程度，刻画了模型本身的拟合能力。 


>  
 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。 


>  
 噪声：表达了当前任务上任何模型所能达到的期望泛化误差的下界，刻画了学习问题本身的难度。 


#### 偏差-方差窘境(bias-variance dilemma)

为了得到泛化性能好的模型，我们需要使偏差较小，即能充分拟合数据，并且使方差小，使数据扰动产生的影响小。但是偏差和方差在一定程度上是有冲突的，这称作为偏差-方差窘境。
-  模型训练不足时，拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导泛化误差，此时称为欠拟合现象。 -  当随着训练程度加深，模型的拟合能力增强，训练数据的扰动慢慢使得方差主导泛化误差。当训练充足时，模型的拟合能力非常强，数据轻微变化都能导致模型发生变化，如果过分学习训练数据的特点，则会发生过拟合。 
针对欠拟合：

我们提出集成学习的概念并且对于模型可以控制训练程度，比如神经网络加多隐层，或者决策树增加树深。 增加模型的迭代次数；更换描述能力更强的模型；生成更多特征供训练使用；降低正则化水平。

针对过拟合：

我们需要降低模型的复杂度，提出了正则化惩罚项。 扩增训练集；减少训练使用的特征的数量；提高正则化水平。 随着模型复杂度的提升, 偏差逐渐减小, 方差逐渐增大. 最佳的模型复杂度是在 Total Error 最小的时候, 该点导数为0.

偏差：实际上也可以称为避免欠拟合。

1、寻找更好的特征 – 具有代表性。

2、用更多的特征 – 增大输入向量的维度。（增加模型复杂度）

方差：避免过拟合

1、增大数据集合 – 使用更多的数据，噪声点比减少（减少数据扰动所造成的影响（紧扣定义））

2、减少数据特征 – 减少数据维度，高维空间密度小（减少模型复杂度）

3、正则化方法

4、交叉验证法

>  
 Bagging和Boosting是集成学习当中比较常用的两种方法，刚好分别对应了降低模型方差和偏差。 


>  
 Bagging是通过重采样的方法来得到不同的模型，主要可以降低的是方差。 


>  
 Boosting每一次都关注使得整理的loss减少，很显然可以降低bias。这里的模型之间并不独立，所以不能显著减少variance，而Bagging假设模型独立所以可以减少variance。 


更多内容请参考知乎问题：为什么说bagging是减少variance，而boosting是减少bias?  回答很精彩

参考资料:
1. 周志华《机器学习》1. 1. 