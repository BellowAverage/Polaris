
--- 
title:  Python 处理分析 14 亿条数据实战 
tags: []
categories: [] 

---
你用 Python 处理过的最大数据集有多大？我想大概不会超过上亿条吧，今天分享一个用 Python 处理分析 14 亿条数据的案例。

这份 14 亿条数据集来自 Google Books ，由 Google Ngram viewer 生成，根据书本印刷的每一个年份，记录了一个特定单词或词组在谷歌图书的使用量。数据集中有成千上百万的书，时间上涵盖了从 16 世纪到 2008 年。可以绘制出单词使用量随时间的变化，比如查询「Python 」在历史中出现的频率。



## <img alt="" height="322" src="https://img-blog.csdnimg.cn/87c43ef158284294b5d4a8cd42355ae4.png" width="671">

 

<img alt="" height="313" src="https://img-blog.csdnimg.cn/52aa2a09614e4d42b3cbd2fa3f18b40c.png" width="642">

 

下面使用 Python 的 PyTubes 数据库来加载上面的数据集，然后分析生成上面这样一张图。PyTubes 是一个专门用来加载大数据源的库。

1-gram 的数据集在硬盘上可以展开成为 27 Gb 的数据，这在读入 python 时是一个很大的数据量级。Python可以轻易地一次性地处理千兆的数据，但是当数据是损坏的和已加工的，速度就会变慢而且内存效率也会变低。

总的来说，这 14 亿条数据（1,430,727,243）分散在 38 个源文件中，一共有 2 千 4 百万个（24,359,460）单词（和词性标注，见下方），计算自 1505 年至 2008 年。

当处理 10 亿行数据时，速度会很快变慢。并且原生 Python 并没有处理这方面数据的优化。幸运的是，numpy 真的很擅长处理大体量数据。使用一些简单的技巧，我们可以使用 numpy 让这个分析变得可行。

在 python/numpy 中处理字符串很复杂。字符串在 python 中的内存开销是很显著的，并且 numpy 只能够处理长度已知而且固定的字符串。基于这种情况，大多数的单词有不同的长度，因此这并不理想。

#### **加载数据集**

> 
  下面所有的代码/例子都是运行在  
 **8 GB 内存** 的 2016 年的 Macbook Pro。如果硬件或云实例有更好的 ram 配置，表现

