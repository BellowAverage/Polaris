
--- 
title:  å°ç›®æ ‡æ£€æµ‹3_æ³¨æ„åŠ›æœºåˆ¶_Self-Attention 
tags: []
categories: [] 

---
ä¸»è¦å‚è€ƒï¼š   

ï¼ˆæ–‡ä¸­æˆªå›¾å¤šæ¥æºäºä¸Šè¿°é“¾æ¥ï¼‰ æ”¹è¿›éƒ¨åˆ†å‚è€ƒï¼š  



#### æ–‡ç« ç›®å½•
- - - - - <ul><li>- <ul><li>- - - 


## æ³¨æ„åŠ›æœºåˆ¶

<img src="https://img-blog.csdnimg.cn/fa2f4874f0b1415aba07ed5b170513b9.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

å› æ­¤ï¼Œ** â€œæ˜¯å¦åŒ…å«â¾ƒä¸»æ€§æâ½° â€ ** å°† æ³¨æ„â¼’æœºåˆ¶ ä¸ å…¨è¿æ¥å±‚æˆ–æ±‡èšå±‚ åŒºåˆ«å¼€æ¥ã€‚ åœ¨æ³¨æ„â¼’æœºåˆ¶çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å°†**â¾ƒä¸»æ€§æâ½°ç§°ä¸ºæŸ¥è¯¢ï¼ˆqueryï¼‰**ã€‚ç»™å®šä»»ä½•æŸ¥è¯¢ï¼Œæ³¨æ„â¼’æœºåˆ¶é€šè¿‡æ³¨æ„â¼’æ±‡èšï¼ˆattention poolingï¼‰å°†é€‰æ‹©å¼•å¯¼â¾„æ„Ÿå®˜è¾“â¼Šï¼ˆsensory inputsï¼Œä¾‹å¦‚ä¸­é—´ç‰¹å¾è¡¨â½°ï¼‰ã€‚åœ¨æ³¨æ„â¼’æœºåˆ¶ä¸­ï¼Œè¿™äº›**æ„Ÿå®˜è¾“â¼Šè¢«ç§°ä¸ºå€¼ï¼ˆvalueï¼‰**ã€‚æ›´é€šä¿—çš„è§£é‡Šï¼Œ**æ¯ä¸ªå€¼éƒ½ä¸â¼€ä¸ªé”®ï¼ˆkeyï¼‰é…å¯¹**ï¼Œè¿™å¯ä»¥æƒ³è±¡ä¸ºæ„Ÿå®˜è¾“â¼Šçš„**â¾®â¾ƒä¸»æâ½°**ã€‚å¦‚ å›¾10.1.3æ‰€â½°ï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡æ³¨æ„â¼’æ±‡èšï¼Œä»¥ä¾¿ç»™å®šçš„æŸ¥è¯¢ï¼ˆâ¾ƒä¸»æ€§æâ½°ï¼‰å¯ä»¥ä¸é”®ï¼ˆâ¾®â¾ƒä¸»æ€§æâ½°ï¼‰è¿›â¾åŒ¹é…ï¼Œè¿™å°†å¼•å¯¼å¾—å‡ºæœ€åŒ¹é…çš„å€¼ï¼ˆæ„Ÿå®˜è¾“â¼Šï¼‰ã€‚

<img src="https://img-blog.csdnimg.cn/7df9075c595b44ac8c1287d4e11bb0c8.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

## æ³¨æ„åŠ›æ±‡èšï¼ˆæ± åŒ–ï¼‰attention pooling

ä¸€èˆ¬æ˜¯ä½¿ç”¨ ä¸€å±‚ç½‘ç»œï¼ˆæ³¨æ„åŠ›æ±‡èš æˆ– æ³¨æ„åŠ›æ± åŒ–ï¼‰å®ç°æ³¨æ„åŠ›æœºåˆ¶ã€‚ æˆ‘ä»¬å¸Œæœ›è·å¾—æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€šè¿‡å­¦ä¹ åˆ°çš„**æ³¨æ„åŠ›æƒé‡**è¡¨ç¤ºï¼‰

é€šâ½¤çš„æ³¨æ„â¼’æ±‡èšï¼ˆattention poolingï¼‰å…¬å¼ï¼š <img src="https://img-blog.csdnimg.cn/c964ee82a92c48209ba1a519f43198de.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> xâ€”â€”â€”â€”æŸ¥è¯¢ query xiâ€”â€”â€”â€”é”® key yiâ€”â€”â€”â€”å€¼ value (xi, yi)â€”â€”é”®å€¼å¯¹ **æ³¨æ„â¼’æ±‡èš f(x)**æ˜¯yiçš„åŠ æƒå¹³å‡ã€‚å°† **æŸ¥è¯¢xå’Œé”®xi**ä¹‹é—´çš„ å…³ç³»ï¼ˆæˆ–è€…ç§°ä¸ºä¸keyå¯¹åº”çš„valueçš„æ¦‚ç‡åˆ†å¸ƒï¼‰å»ºæ¨¡ä¸º **æ³¨æ„â¼’æƒé‡ï¼ˆattention weightï¼‰Î±(x, xi)**ã€‚è¿™ä¸ªæƒé‡å°†è¢«åˆ†é…ç»™æ¯â¼€ä¸ªå¯¹åº”å€¼yiã€‚`â€œæŸ¥è¯¢-é”®â€å¯¹è¶Šæ¥è¿‘ï¼Œè¶Šå…·æœ‰å‚è€ƒä»·å€¼ï¼Œæ³¨æ„â¼’æƒé‡å°±è¶Šâ¾¼` <img src="https://img-blog.csdnimg.cn/6c9261d4ca0248d59c7f60bfc299ebba.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

å¯¹äºä»»ä½•æŸ¥è¯¢xï¼Œæ¨¡å‹åœ¨æ‰€æœ‰é”®å€¼å¯¹æ³¨æ„â¼’æƒé‡éƒ½æ˜¯â¼€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒï¼šå®ƒä»¬æ˜¯â¾®è´Ÿçš„ï¼Œå¹¶ä¸”æ€»å’Œä¸º1ï¼ˆå½±åˆºä½¿ç”¨softmaxå‡½æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼‰ã€‚

å¸¦å‚æ•°æ³¨æ„â¼’æ±‡èšï¼šåœ¨ä¸‹â¾¯çš„æŸ¥è¯¢xå’Œé”®xiä¹‹é—´çš„è·ç¦»ä¹˜ä»¥`å¯å­¦ä¹ å‚æ•°w` <img src="https://img-blog.csdnimg.cn/31fc45654aa1433693e8abecf091a1bd.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

**æ³¨æ„åŠ›æœºåˆ¶æ¡†æ¶ï¼š** <img src="https://img-blog.csdnimg.cn/2a37cd0d538045a8af25c6b4a2d02897.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://img-blog.csdnimg.cn/9cd25a7f54de415b9bda431a0d49e593.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

## è‡ªæ³¨æ„åŠ› Self-Attention

<img src="https://img-blog.csdnimg.cn/51413c2c49b04d67b434687e439351c4.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">
- æ¯ä¸€ä¸ªoutptéƒ½è¦è€ƒè™‘æ•´ä¸ªsequence- ä¸éœ€è¦ä¾åºç”Ÿæˆoutputï¼ˆå¯ä»¥åŒæ—¶è®¡ç®—ï¼‰
**åŠ æ€§æ³¨æ„â¼’ï¼š** <img src="https://img-blog.csdnimg.cn/e531a199f3794e7c92ab37f89311600e.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

ä¸‹é¢æ˜¯æå®æ¯…è€å¸ˆçš„PPTå†…å®¹<img src="https://img-blog.csdnimg.cn/55f1121968bb4a71b1f420d047e45ec4.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://img-blog.csdnimg.cn/5bd92dcef7264ea28edee63e28d34f59.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å†™æˆçŸ©é˜µçš„å½¢å¼ï¼š <img src="https://img-blog.csdnimg.cn/60da8ff855cd44ccbe6f8f4f88897d88.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://img-blog.csdnimg.cn/43f9e35e265e4428be836c0dc9d382f0.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://img-blog.csdnimg.cn/c51c52f0f0cf479293cc2c4b76aea035.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://img-blog.csdnimg.cn/13a22539cadd43e48dba876e98356662.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://img-blog.csdnimg.cn/2a45bdc3aa9345e599489bf92f9211c0.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://img-blog.csdnimg.cn/10a370f0fe53496dbe19f684ac83d273.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

## CNNä¸­çš„Self-Attention

<img src="https://img-blog.csdnimg.cn/db4f322116014f68a9a91ac73ca72db1.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å°†æ¯ä¸€ä¸ªåƒç´ ç‚¹çœ‹æˆä¸€ä¸ªå‘é‡ï¼ŒåŒ…å«å¤šä¸ªé€šé“ã€‚ <img src="https://img-blog.csdnimg.cn/0167107f68bf4ac4a226694dd5341949.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

### Channel attention

<img src="https://img-blog.csdnimg.cn/b8c770f91a5a43b98c27789a1aa77de6.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

#### 1. SENet

è®ºæ–‡åç§°ï¼šSqueeze-and-Excitation Networks

è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/1709.01507.pdf

è®ºæ–‡ä»£ç ï¼š https://github.com/hujie-frank/SENet

SEnetï¼ˆSqueeze-and-Excitation Networkï¼‰ï¼Œ2017å¹´æå‡ºçš„SENetæ˜¯æœ€åä¸€å±ŠImageNetç«èµ›çš„å† å†›ï¼Œè€ƒè™‘äº†ç‰¹å¾é€šé“ä¹‹é—´çš„å…³ç³»ï¼Œåœ¨**ç‰¹å¾é€šé“**ä¸ŠåŠ å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ã€‚

SEneté€šè¿‡å­¦ä¹ çš„æ–¹å¼è‡ªåŠ¨è·å–æ¯ä¸ªç‰¹å¾é€šé“çš„é‡è¦ç¨‹åº¦ã€‚ å¯¹äºè¾“å…¥è¿›æ¥çš„ç‰¹å¾å±‚ï¼Œå…³æ³¨å…¶æ¯ä¸€ä¸ªé€šé“çš„æƒé‡ï¼Œå¯¹äºSENetè€Œè¨€ï¼Œå…¶é‡ç‚¹æ˜¯è·å¾—è¾“å…¥è¿›æ¥çš„ç‰¹å¾å±‚ï¼Œæ¯ä¸€ä¸ªé€šé“çš„æƒå€¼ã€‚ åˆ©ç”¨SENetå¾—åˆ°çš„é‡è¦ç¨‹åº¦ï¼Œæ¥æå‡ç‰¹å¾å¹¶æŠ‘åˆ¶å¯¹å½“å‰ä»»åŠ¡ä¸é‡è¦çš„ç‰¹å¾ï¼Œè®©ç½‘ç»œå…³æ³¨å®ƒæœ€éœ€è¦å…³æ³¨çš„é€šé“ã€‚ <img src="https://img-blog.csdnimg.cn/efd0b3a3e7b14efc91b0b0abfe3e5636.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å…¶å…·ä½“å®ç°æ–¹å¼å°±æ˜¯ï¼š 1ã€å¯¹è¾“å…¥è¿›æ¥çš„ç‰¹å¾å±‚è¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–ã€‚ 2ã€ç„¶åè¿›è¡Œä¸¤æ¬¡å…¨è¿æ¥ï¼Œç¬¬ä¸€æ¬¡å…¨è¿æ¥ç¥ç»å…ƒä¸ªæ•°è¾ƒå°‘ï¼Œç¬¬äºŒæ¬¡å…¨è¿æ¥ç¥ç»å…ƒä¸ªæ•°å’Œè¾“å…¥ç‰¹å¾å±‚ç›¸åŒã€‚ 3ã€åœ¨å®Œæˆä¸¤æ¬¡å…¨è¿æ¥åï¼Œæˆ‘ä»¬å†å–ä¸€æ¬¡Sigmoidå°†å€¼å›ºå®šåˆ°0-1ä¹‹é—´ï¼Œæ­¤æ—¶æˆ‘ä»¬è·å¾—äº†è¾“å…¥ç‰¹å¾å±‚æ¯ä¸€ä¸ªé€šé“çš„æƒå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰ã€‚ 4ã€åœ¨è·å¾—è¿™ä¸ªæƒå€¼åï¼Œæˆ‘ä»¬å°†è¿™ä¸ªæƒå€¼**ä¹˜ä¸Š**åŸè¾“å…¥ç‰¹å¾å±‚å³å¯ã€‚(å¯¹è¾“å…¥ç‰¹å¾çš„é€šé“è¿›è¡Œ**ç¼©æ”¾**ï¼‰

>  
 Global spatial information is collected in the squeeze module by global average pooling excitation module captures channel-wise relationships and outputs an attention vector by using fully-connected layers and non-linear layers (ReLU and sigmoid). 


```
class se_block(nn.Module):
    def __init__(self, channel, ratio=16):
        super(se_block, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
                nn.Linear(channel, channel // ratio, bias=False),
                nn.ReLU(inplace=True),
                nn.Linear(channel // ratio, channel, bias=False),
                nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

```

ç¼ºç‚¹ï¼š åœ¨squeeze moduleæŒ¤å‹æ¨¡å—ä¸­ï¼Œå…¨å±€å¹³å‡æ± è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰å¤æ‚çš„å…¨å±€ä¿¡æ¯ã€‚ åœ¨excitation moduleæ¿€åŠ±æ¨¡å—ä¸­ï¼Œå…¨è¿é€šå±‚å¢åŠ äº†æ¨¡å‹çš„å¤æ‚æ€§ã€‚

#### 2. ECA

è®ºæ–‡åç§°ï¼šECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks

è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/1910.03151

ä»£ç : https://github.com/BangguWu/ECANet

ECANetå¯ä»¥çœ‹ä½œæ˜¯SENetçš„æ”¹è¿›ç‰ˆã€‚ ECANetçš„ä½œè€…è®¤ä¸ºSENetå¯¹é€šé“æ³¨æ„åŠ›æœºåˆ¶çš„é¢„æµ‹å¸¦æ¥äº†å‰¯ä½œç”¨ï¼Œæ•è·æ‰€æœ‰é€šé“çš„ä¾èµ–å…³ç³»æ˜¯ä½æ•ˆå¹¶ä¸”æ˜¯ä¸å¿…è¦çš„ã€‚ åœ¨ECANetçš„è®ºæ–‡ä¸­ï¼Œä½œè€…è®¤ä¸º**å·ç§¯å…·æœ‰è‰¯å¥½çš„è·¨é€šé“ä¿¡æ¯è·å–èƒ½åŠ›**ã€‚

ECAæ¨¡å—çš„æ€æƒ³æ˜¯éå¸¸ç®€å•çš„ï¼Œå®ƒå»é™¤äº†åŸæ¥SEæ¨¡å—ä¸­çš„å…¨è¿æ¥å±‚ï¼Œç›´æ¥åœ¨å…¨å±€å¹³å‡æ± åŒ–ä¹‹åçš„ç‰¹å¾ä¸Šé€šè¿‡ä¸€ä¸ª**1Då·ç§¯è¿›è¡Œå­¦ä¹ **ã€‚

>  
 æ—¢ç„¶ä½¿ç”¨åˆ°äº†1Då·ç§¯ï¼Œé‚£ä¹ˆ1Då·ç§¯çš„å·ç§¯æ ¸å¤§å°çš„é€‰æ‹©å°±å˜å¾—éå¸¸é‡è¦äº†ï¼Œäº†è§£è¿‡å·ç§¯åŸç†çš„åŒå­¦å¾ˆå¿«å°±å¯ä»¥æ˜ç™½ï¼Œ1Då·ç§¯çš„å·ç§¯æ ¸å¤§å°ä¼šå½±å“æ³¨æ„åŠ›æœºåˆ¶æ¯ä¸ªæƒé‡çš„è®¡ç®—è¦è€ƒè™‘çš„é€šé“æ•°é‡ã€‚ç”¨æ›´ä¸“ä¸šçš„åè¯å°±æ˜¯è·¨é€šé“äº¤äº’çš„è¦†ç›–ç‡ã€‚ 


å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå·¦å›¾æ˜¯å¸¸è§„çš„SEæ¨¡å—ï¼Œå³å›¾æ˜¯ECAæ¨¡å—ã€‚ECAæ¨¡å—ç”¨1Då·ç§¯æ›¿æ¢ä¸¤æ¬¡å…¨è¿æ¥ã€‚ <img src="https://img-blog.csdnimg.cn/449378bd92594ccfab2537a9dbb5b8e0.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

```
class eca_block(nn.Module):
    def __init__(self, channel, b=1, gamma=2):
        super(eca_block, self).__init__()
        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) 
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        return x * y.expand_as(x)


```

#### CA æ³¨æ„åŠ›æ¨¡å—

å‚è€ƒåšå®¢ï¼šhttps://blog.csdn.net/weixin_43694096/article/details/124443059

å…ˆå‰çš„è½»é‡çº§ç½‘ç»œçš„æ³¨æ„åŠ›æœºåˆ¶å¤§éƒ½é‡‡ç”¨SEæ¨¡å—ï¼Œä»…è€ƒè™‘äº†é€šé“é—´çš„ä¿¡æ¯ï¼Œå¿½ç•¥äº†ä½ç½®ä¿¡æ¯ã€‚å°½ç®¡åæ¥çš„BAMå’ŒCBAMå°è¯•åœ¨é™ä½é€šé“æ•°åé€šè¿‡å·ç§¯æ¥æå–ä½ç½®æ³¨æ„åŠ›ä¿¡æ¯ï¼Œä½†å·ç§¯åªèƒ½æå–**å±€éƒ¨**å…³ç³»ï¼Œç¼ºä¹é•¿è·ç¦»å…³ç³»æå–çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†æ–°çš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶**coordinate attentionï¼ˆCAï¼‰**ï¼Œèƒ½å¤Ÿå°†æ¨ªå‘å’Œçºµå‘çš„ä½ç½®ä¿¡æ¯ç¼–ç åˆ°channel attentionä¸­ï¼Œä½¿å¾—ç§»åŠ¨ç½‘ç»œèƒ½å¤Ÿå…³æ³¨å¤§èŒƒå›´çš„ä½ç½®ä¿¡æ¯åˆä¸ä¼šå¸¦æ¥è¿‡å¤šçš„è®¡ç®—é‡ã€‚

coordinate attentionçš„ä¼˜åŠ¿ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š ä¸ä»…è·å–äº†é€šé“é—´ä¿¡æ¯ï¼Œè¿˜è€ƒè™‘äº†æ–¹å‘ç›¸å…³çš„ä½ç½®ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°å®šä½å’Œè¯†åˆ«ç›®æ ‡ï¼› è¶³å¤Ÿçµæ´»å’Œè½»é‡ï¼Œèƒ½å¤Ÿç®€å•åœ°æ’å…¥ç§»åŠ¨ç½‘ç»œçš„æ ¸å¿ƒç»“æ„ä¸­ï¼› å¯ä»¥ä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ç”¨äºå¤šç§ä»»åŠ¡ä¸­ï¼Œå¦‚æ£€æµ‹å’Œåˆ†å‰²ï¼Œå‡æœ‰ä¸é”™çš„æ€§èƒ½æå‡ã€‚

<img src="https://img-blog.csdnimg.cn/86e23d69048941e58f048f723d983925.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> æ®‹å·®ç»“æ„

```
# CA
class h_sigmoid(nn.Module):
    def __init__(self, inplace=True):
        super(h_sigmoid, self).__init__()
        self.relu = nn.ReLU6(inplace=inplace)
    def forward(self, x):
        return self.relu(x + 3) / 6
class h_swish(nn.Module):
    def __init__(self, inplace=True):
        super(h_swish, self).__init__()
        self.sigmoid = h_sigmoid(inplace=inplace)
    def forward(self, x):
        return x * self.sigmoid(x)

class CoordAtt(nn.Module):
    def __init__(self, inp, oup, reduction=32):
        super(CoordAtt, self).__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        mip = max(8, inp // reduction)
        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = h_swish()
        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
    def forward(self, x):
        identity = x
        n, c, h, w = x.size()
        #c*1*W
        x_h = self.pool_h(x)
        #c*H*1
        #C*1*h
        x_w = self.pool_w(x).permute(0, 1, 3, 2)
        y = torch.cat([x_h, x_w], dim=2)
        #C*1*(h+w)
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)
        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()
        out = identity * a_w * a_h
        return out


```

### Channel &amp; Spatial attention

<img src="https://img-blog.csdnimg.cn/4510c289fb1041299bae437bf90986e8.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

#### 1. CBAM

è®ºæ–‡é¢˜ç›®ï¼šã€ŠCBAM: Convolutional Block Attention Moduleã€‹

è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/pdf/1807.06521.pdf

CBAM(Convolutional Block Attention Module)ç»“åˆäº†ç‰¹å¾é€šé“å’Œç‰¹å¾ç©ºé—´ä¸¤ä¸ªç»´åº¦çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

CBAMå°†**é€šé“æ³¨æ„åŠ›æœºåˆ¶**å’Œ**ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶**è¿›è¡Œä¸€ä¸ªç»“åˆï¼Œç›¸æ¯”äºSENetåªå…³æ³¨é€šé“çš„æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å–å¾—æ›´å¥½çš„æ•ˆæœã€‚å…¶å®ç°ç¤ºæ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼ŒCBAMä¼šå¯¹è¾“å…¥è¿›æ¥çš„ç‰¹å¾å±‚ï¼Œåˆ†åˆ«è¿›è¡Œé€šé“æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†å’Œç©ºé—´æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†ã€‚ <img src="https://img-blog.csdnimg.cn/773277d4b36a47219363f016a054ea4a.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> ä¸‹å›¾æ˜¯é€šé“æ³¨æ„åŠ›æœºåˆ¶å’Œç©ºé—´æ³¨æ„åŠ›æœºåˆ¶çš„å…·ä½“å®ç°æ–¹å¼ï¼š

é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼š <img src="https://img-blog.csdnimg.cn/88819cb054314e71a1886b7e7e57d305.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> é€šé“æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šå¯¹è¾“å…¥è¿›æ¥çš„å•ä¸ªç‰¹å¾å±‚ï¼Œåˆ†åˆ«è¿›è¡Œ**å…¨å±€å¹³å‡æ± åŒ–**å’Œ**å…¨å±€æœ€å¤§æ± åŒ–**ã€‚ä¹‹åå¯¹å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–çš„ç»“æœï¼Œåˆ©ç”¨**å…±äº«çš„å…¨è¿æ¥å±‚**è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬ä¼šå¯¹å¤„ç†åçš„ä¸¤ä¸ªç»“æœè¿›è¡Œ**ç›¸åŠ **ï¼Œç„¶åå–ä¸€ä¸ª**sigmoid**ï¼Œæ­¤æ—¶æˆ‘ä»¬è·å¾—äº†è¾“å…¥ç‰¹å¾å±‚æ¯ä¸€ä¸ªé€šé“çš„**æƒå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰**ã€‚åœ¨è·å¾—è¿™ä¸ªæƒå€¼åï¼Œæˆ‘ä»¬å°†è¿™ä¸ª**æƒå€¼ä¹˜ä¸ŠåŸè¾“å…¥ç‰¹å¾å±‚**å³å¯ã€‚

ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼š <img src="https://img-blog.csdnimg.cn/7043a30c10c34a0cb5254248099a0d50.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">

æˆ‘ä»¬ä¼šå¯¹è¾“å…¥è¿›æ¥çš„ç‰¹å¾å±‚ï¼Œåœ¨æ¯ä¸€ä¸ªç‰¹å¾ç‚¹çš„é€šé“ä¸Šå–**æœ€å¤§å€¼**å’Œ**å¹³å‡å€¼**ã€‚ä¹‹åå°†è¿™ä¸¤ä¸ªç»“æœè¿›è¡Œä¸€ä¸ª**å †å **ï¼Œåˆ©ç”¨ä¸€æ¬¡é€šé“æ•°ä¸º1çš„å·ç§¯è°ƒæ•´é€šé“æ•°ï¼Œç„¶åå–ä¸€ä¸ªsigmoidï¼Œæ­¤æ—¶æˆ‘ä»¬è·å¾—äº†è¾“å…¥ç‰¹å¾å±‚æ¯ä¸€ä¸ªç‰¹å¾ç‚¹çš„**æƒå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰**ã€‚åœ¨è·å¾—è¿™ä¸ªæƒå€¼åï¼Œæˆ‘ä»¬å°†è¿™ä¸ª**æƒå€¼ä¹˜ä¸ŠåŸè¾“å…¥ç‰¹å¾å±‚**å³å¯ã€‚

```
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=8):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # åˆ©ç”¨1x1å·ç§¯ä»£æ›¿å…¨è¿æ¥
        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class cbam_block(nn.Module):
    def __init__(self, channel, ratio=8, kernel_size=7):
        super(cbam_block, self).__init__()
        self.channelattention = ChannelAttention(channel, ratio=ratio)
        self.spatialattention = SpatialAttention(kernel_size=kernel_size)

    def forward(self, x):
        x = x * self.channelattention(x)
        x = x * self.spatialattention(x)
        return x


```

## æ³¨æ„åŠ›æœºåˆ¶çš„åº”ç”¨

æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œç†è®ºä¸Šå¯ä»¥æ”¾åœ¨ä»»ä½•ä¸€ä¸ªç‰¹å¾å±‚åé¢ï¼Œå¯ä»¥æ”¾åœ¨ä¸»å¹²ç½‘ç»œï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨åŠ å¼ºç‰¹å¾æå–ç½‘ç»œã€‚

ç”±äºæ”¾ç½®åœ¨ä¸»å¹²ä¼šå¯¼è‡´ç½‘ç»œçš„**é¢„è®­ç»ƒæƒé‡æ— æ³•ä½¿ç”¨**ï¼Œ è‹¥æƒ³ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå¯ä»¥å°†æ³¨æ„åŠ›æœºåˆ¶åº”ç”¨**åŠ å¼ºç‰¹å¾æå–ç½‘ç»œ**ä¸Šã€‚

ShuffleAttentionæ³¨æ„åŠ›æœºåˆ¶ CrissCrossAttentionæ³¨æ„åŠ›æœºåˆ¶ S2-MLPv2æ³¨æ„åŠ›æœºåˆ¶ SimAMæ³¨æ„åŠ›æœºåˆ¶ SKAttentionæ³¨æ„åŠ›æœºåˆ¶ NAMAttentionæ³¨æ„åŠ›æœºåˆ¶ SOCAæ³¨æ„åŠ›æœºåˆ¶ CBAMæ³¨æ„åŠ›æœºåˆ¶ SEAttentionæ³¨æ„åŠ›æœºåˆ¶ GMAttentionæ³¨æ„åŠ›æœºåˆ¶ CAæ³¨æ„åŠ›æœºåˆ¶

åšå®¢é“¾æ¥ğŸ”—ğŸŒŸï¼šgithub

YOLOv5 + ECAæ³¨æ„åŠ›æœºåˆ¶ åšå®¢é“¾æ¥ğŸ”—ğŸŒŸï¼šgithub â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºCSDNåšä¸»ã€ŒèŠ’æœæ±æ²¡æœ‰èŠ’æœã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚ åŸæ–‡é“¾æ¥ï¼šhttps://blog.csdn.net/qq_38668236/article/details/126073577
